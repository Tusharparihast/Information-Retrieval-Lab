{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69db54f3",
   "metadata": {},
   "source": [
    "# Lab 1: Text Preprocessing, Indexing, and Compression\n",
    "\n",
    "### Objective\n",
    "To perform text preprocessing on a small document collection, analyze its effect on vocabulary size and term frequencies, construct inverted and positional indexes, and apply compression techniques on posting lists.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84e1ecd",
   "metadata": {},
   "source": [
    "## Document Collection\n",
    "\n",
    "We consider a collection of three short documents:\n",
    "\n",
    "- **D1**: \"Information retrieval is the process of obtaining relevant information.\"\n",
    "- **D2**: \"Retrieval systems use indexing and ranking techniques.\"\n",
    "- **D3**: \"Information systems retrieve data efficiently.\"\n",
    "\n",
    "Each document is assigned a unique document ID.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97f30339",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = {\n",
    "    1: \"Information retrieval is the process of obtaining relevant information\",\n",
    "    2: \"Retrieval systems use indexing and ranking techniques\",\n",
    "    3: \"Information systems retrieve data efficiently\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e686e404",
   "metadata": {},
   "source": [
    "## Text Preprocessing\n",
    "\n",
    "The preprocessing pipeline consists of:\n",
    "1. Tokenization\n",
    "2. Stopword removal\n",
    "3. Stemming\n",
    "4. Lemmatization\n",
    "\n",
    "Vocabulary size is defined as the number of unique terms in the document collection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1891ed74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Required librarary components for text processing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from collections import Counter\n",
    "\n",
    "#Downlaod necessary NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess(text):\n",
    "    tokens = text.lower().split()\n",
    "    tokens = [t for t in tokens if t not in stop_words]\n",
    "    stemmed = [stemmer.stem(t) for t in tokens]\n",
    "    lemmatized = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "    return tokens, stemmed, lemmatized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6944fcac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Doc                                                               Original                                              Processed (Lemmatized)\n",
      "   1 Information retrieval is the process of obtaining relevant information [information, retrieval, process, obtaining, relevant, information]\n",
      "   2                  Retrieval systems use indexing and ranking techniques              [retrieval, system, use, indexing, ranking, technique]\n",
      "   3                          Information systems retrieve data efficiently                  [information, system, retrieve, data, efficiently]\n",
      "\n",
      "--- Analysis ---\n",
      "Original Vocabulary Size: 17\n",
      "Preprocessed Vocabulary Size: 13\n",
      "Reduction: 23.53%\n",
      "\n",
      "Top 5 Terms (Raw): [('information', 3), ('retrieval', 2), ('systems', 2), ('is', 1), ('the', 1)]\n",
      "Top 5 Terms (Processed): [('information', 3), ('retrieval', 2), ('system', 2), ('process', 1), ('obtaining', 1)]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Helper function for raw tokenization (no preprocessing)\n",
    "def raw_tokenize(text):\n",
    "    return re.findall(r'\\b\\w+\\b', text.lower())\n",
    "\n",
    "original_tokens = []\n",
    "processed_tokens = []\n",
    "rows = []\n",
    "processed_docs = {}\n",
    "\n",
    "# Process each document\n",
    "for doc_id, doc in documents.items():\n",
    "    # Raw tokens\n",
    "    raw = raw_tokenize(doc)\n",
    "    original_tokens.extend(raw)\n",
    "\n",
    "    # Preprocessed tokens (lemmatized)\n",
    "    tokens, _, lemmas = preprocess(doc)\n",
    "    processed_tokens.extend(lemmas)\n",
    "    processed_docs[doc_id] = lemmas\n",
    "\n",
    "    # Store row for comparison table\n",
    "    rows.append([\n",
    "        doc_id,\n",
    "        doc,\n",
    "        lemmas\n",
    "    ])\n",
    "\n",
    "# Display document-wise comparison\n",
    "df = pd.DataFrame(\n",
    "    rows,\n",
    "    columns=[\"Doc\", \"Original\", \"Processed (Lemmatized)\"]\n",
    ")\n",
    "\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "# Vocabulary statistics\n",
    "original_vocab = set(original_tokens)\n",
    "processed_vocab = set(processed_tokens)\n",
    "\n",
    "original_tf = Counter(original_tokens)\n",
    "processed_tf = Counter(processed_tokens)\n",
    "\n",
    "reduction = (1 - len(processed_vocab) / len(original_vocab)) * 100\n",
    "\n",
    "\n",
    "print(\"\\n--- Analysis ---\")\n",
    "print(f\"Original Vocabulary Size: {len(original_vocab)}\")\n",
    "print(f\"Preprocessed Vocabulary Size: {len(processed_vocab)}\")\n",
    "print(f\"Reduction: {reduction:.2f}%\")\n",
    "\n",
    "print(\"\\nTop 5 Terms (Raw):\", original_tf.most_common(5))\n",
    "print(\"Top 5 Terms (Processed):\", processed_tf.most_common(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c12b52",
   "metadata": {},
   "source": [
    "## Inverted Index Construction\n",
    "\n",
    "- An inverted index maps terms to the documents in which they occur.\n",
    "- A positional index additionally stores the position of each term in a document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f03ab111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term            | Postings List (DocID) | Positional Index (DocID: [Positions])\n",
      "------------------------------------------------------------------------------------------\n",
      "data            | [3]                  | {3: [3]}\n",
      "efficiently     | [3]                  | {3: [4]}\n",
      "indexing        | [2]                  | {2: [3]}\n",
      "information     | [1, 3]               | {1: [0, 5], 3: [0]}\n",
      "obtaining       | [1]                  | {1: [3]}\n",
      "process         | [1]                  | {1: [2]}\n",
      "ranking         | [2]                  | {2: [4]}\n",
      "relevant        | [1]                  | {1: [4]}\n",
      "retrieval       | [1, 2]               | {1: [1], 2: [0]}\n",
      "retrieve        | [3]                  | {3: [2]}\n",
      "system          | [2, 3]               | {2: [1], 3: [1]}\n",
      "technique       | [2]                  | {2: [5]}\n",
      "use             | [2]                  | {2: [2]}\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "class InvertedIndex:\n",
    "    def __init__(self):\n",
    "        # Basic inverted index: term -> list of document IDs\n",
    "        self.index = defaultdict(list)\n",
    "\n",
    "        # Positional inverted index: term -> {doc_id: [pos1, pos2, ...]}\n",
    "        self.positional_index = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "    def build(self, docs):\n",
    "        for doc_id, tokens in docs.items():\n",
    "            for pos, term in enumerate(tokens):\n",
    "                # Basic Inverted Index (Avoid duplicate document IDs in posting list)\n",
    "                if doc_id not in self.index[term]:\n",
    "                    self.index[term].append(doc_id)\n",
    "\n",
    "                # Positional Inverted Index (Store positions of terms in documents)\n",
    "                self.positional_index[term][doc_id].append(pos)\n",
    "\n",
    "    def get_postings(self, term):\n",
    "        return self.index.get(term, [])\n",
    "\n",
    "    def get_positional_postings(self, term):\n",
    "        return self.positional_index.get(term, {})\n",
    "    \n",
    "# Build the index\n",
    "ir_system = InvertedIndex()\n",
    "ir_system.build(processed_docs)\n",
    "\n",
    "# Display the inverted index & positional index\n",
    "print(f\"{'Term':<15} | {'Postings List (DocID)':<20} | {'Positional Index (DocID: [Positions])'}\")\n",
    "print(\"-\" * 90)\n",
    "# Sort terms alphabetically for clean output\n",
    "sorted_terms = sorted(ir_system.index.keys())\n",
    "\n",
    "for term in sorted_terms:\n",
    "    postings = ir_system.index[term]\n",
    "    positional = dict(ir_system.positional_index[term])\n",
    "    print(f\"{term:<15} | {str(postings):<20} | {positional}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0043687f",
   "metadata": {},
   "source": [
    "## Query Processing\n",
    "\n",
    "- **Single-term query**: Find docs containing \"information\".\n",
    "- **Boolean query**: \"(data AND retrieve)NOT indexing\", \"information OR use\".\n",
    "- **Phrase query**: \"retrieve data\". Requires checking if positions are adjacent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a4687ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SINGLE-TERM QUERY: 'information'\n",
      "--------------------------------------------------\n",
      "Postings(information) = [1, 3]\n",
      "\n",
      "BOOLEAN QUERY: (data AND retrieve) NOT indexing\n",
      "--------------------------------------------------\n",
      "Postings(data) = [3]\n",
      "Postings(retrieve) = [3]\n",
      "Postings(indexing) = [2]\n",
      "Final Result = [3]\n",
      "\n",
      "BOOLEAN QUERY: information OR use\n",
      "--------------------------------------------------\n",
      "Postings(information) = [1, 3]\n",
      "Postings(use) = [2]\n",
      "Final Result = []\n",
      "\n",
      "PHRASE QUERY: \"retrieve data\"\n",
      "--------------------------------------------------\n",
      "\n",
      "Document 3:\n",
      "  retrieve positions: [2]\n",
      "  data positions: [3]\n",
      "  → Phrase match found\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def print_postings(term):\n",
    "    postings = set(ir_system.get_postings(term))\n",
    "    print(f\"Postings({term}) = {sorted(postings)}\")\n",
    "    return postings\n",
    "\n",
    "\n",
    "def single_term_query(term):\n",
    "    print(f\"\\nSINGLE-TERM QUERY: '{term}'\")\n",
    "    print(\"-\" * 50)\n",
    "    return print_postings(term)\n",
    "\n",
    "\n",
    "def boolean_query(query_name, include_terms, exclude_terms=None):\n",
    "    print(f\"\\nBOOLEAN QUERY: {query_name}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    result = None\n",
    "\n",
    "    # AND operation for included terms\n",
    "    for term in include_terms:\n",
    "        postings = print_postings(term)\n",
    "        result = postings if result is None else result & postings\n",
    "\n",
    "    # NOT operation for excluded terms\n",
    "    if exclude_terms:\n",
    "        for term in exclude_terms:\n",
    "            exclude_postings = print_postings(term)\n",
    "            result -= exclude_postings\n",
    "\n",
    "    print(f\"Final Result = {sorted(result)}\")\n",
    "    return sorted(result)\n",
    "\n",
    "\n",
    "def phrase_query(term1, term2):\n",
    "    print(f'\\nPHRASE QUERY: \"{term1} {term2}\"')\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    pos1 = ir_system.get_positional_postings(term1)\n",
    "    pos2 = ir_system.get_positional_postings(term2)\n",
    "\n",
    "    result_docs = []\n",
    "\n",
    "    for doc_id in pos1.keys() & pos2.keys():\n",
    "        print(f\"\\nDocument {doc_id}:\")\n",
    "        print(f\"  {term1} positions: {pos1[doc_id]}\")\n",
    "        print(f\"  {term2} positions: {pos2[doc_id]}\")\n",
    "\n",
    "        if any(p + 1 in pos2[doc_id] for p in pos1[doc_id]):\n",
    "            print(\"  → Phrase match found\")\n",
    "            result_docs.append(doc_id)\n",
    "\n",
    "    if not result_docs:\n",
    "        print(\"\\nNo phrase match found.\")\n",
    "\n",
    "    return result_docs\n",
    "\n",
    "# Single-term\n",
    "single_term_query(\"information\")\n",
    "\n",
    "# Boolean\n",
    "boolean_query(\n",
    "    \"(data AND retrieve) NOT indexing\",\n",
    "    include_terms=[\"data\", \"retrieve\"],\n",
    "    exclude_terms=[\"indexing\"]\n",
    ")\n",
    "\n",
    "boolean_query(\n",
    "    \"information OR use\",\n",
    "    include_terms=[\"information\", \"use\"]\n",
    ")\n",
    "\n",
    "# Phrase\n",
    "phrase_query(\"retrieve\", \"data\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2026f9e2",
   "metadata": {},
   "source": [
    "## Index Compression\n",
    "\n",
    "Posting lists are converted to gap representation and then encoded to reduce space.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58eedd8b",
   "metadata": {},
   "source": [
    "### 1. Gap (Delta) Encoding\n",
    "\n",
    "Instead of storing absolute document IDs in posting lists, we store the difference\n",
    "(gap) between consecutive document IDs.\n",
    "\n",
    "Given a sorted posting list:\n",
    "d₁, d₂, d₃, ..., dₙ\n",
    "\n",
    "Gap representation is computed as:\n",
    "gap₁ = d₁  \n",
    "gapᵢ = dᵢ − dᵢ₋₁  for i > 1\n",
    "\n",
    "Since gaps are usually small integers, they are more efficient to encode.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e5b6ecd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term: information\n",
      "Posting List: [1, 3]\n",
      "\n",
      "=== ORIGINAL REPRESENTATION ===\n",
      "Assumption: 32 bits per DocID\n",
      "Total Bits Used: 64\n"
     ]
    }
   ],
   "source": [
    "term = \"information\"\n",
    "posting_list = [1, 3]  \n",
    "\n",
    "print(f\"Term: {term}\")\n",
    "print(f\"Posting List: {posting_list}\")\n",
    "\n",
    "print(\"\\n=== ORIGINAL REPRESENTATION ===\")\n",
    "\n",
    "original_bits = len(posting_list) * 32\n",
    "print(f\"Assumption: 32 bits per DocID\")\n",
    "print(f\"Total Bits Used: {original_bits}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a683d566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== GAP ENCODING ===\n",
      "Gaps: [1, 2]\n",
      "Bits Used (minimal binary representation): 3\n"
     ]
    }
   ],
   "source": [
    "def compute_gaps(posting_list):\n",
    "    if not posting_list:\n",
    "        return []\n",
    "    gaps = [posting_list[0]]\n",
    "    for i in range(1, len(posting_list)):\n",
    "        gaps.append(posting_list[i] - posting_list[i - 1])\n",
    "    return gaps\n",
    "\n",
    "\n",
    "gaps = compute_gaps(posting_list)\n",
    "\n",
    "print(\"\\n=== GAP ENCODING ===\")\n",
    "print(f\"Gaps: {gaps}\")\n",
    "\n",
    "gap_bits = sum(g.bit_length() for g in gaps)\n",
    "print(f\"Bits Used (minimal binary representation): {gap_bits}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0d8213",
   "metadata": {},
   "source": [
    "### 2. Variable-Byte Encoding\n",
    "\n",
    "Variable-Byte encoding represents integers using a variable number of bytes.\n",
    "\n",
    "Each byte:\n",
    "- Uses the most significant bit (MSB) as a continuation flag\n",
    "- MSB = 1 → last byte of the number\n",
    "- MSB = 0 → more bytes follow\n",
    "- Remaining 7 bits store the actual data\n",
    "- Example: 5 (binary 101) fits in 7 bits. Encoded: 10000101 (Hex 85)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "025a4d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== VARIABLE BYTE ENCODING ===\n",
      "Encoded Bytes (Decimal): [129, 130]\n",
      "Encoded Bytes (Hex): ['0x81', '0x82']\n",
      "Encoded Bytes (Binary): ['10000001', '10000010']\n",
      "Total Bits Used: 16\n"
     ]
    }
   ],
   "source": [
    "#Encodes a single integer using VB encoding\n",
    "def vb_encode_number(number):\n",
    "    bytes_list = []\n",
    "    while True:\n",
    "        byte = number % 128\n",
    "        if not bytes_list:\n",
    "            # MSB = 1 indicates last byte\n",
    "            byte += 128\n",
    "        bytes_list.insert(0, byte)\n",
    "        number //= 128\n",
    "        if number == 0:\n",
    "            break\n",
    "    return bytes_list\n",
    "\n",
    "\n",
    "def vb_encode(gaps):\n",
    "    stream = []\n",
    "    for gap in gaps:\n",
    "        stream.extend(vb_encode_number(gap))\n",
    "    return stream\n",
    "\n",
    "\n",
    "vb_encoded = vb_encode(gaps)\n",
    "\n",
    "print(\"\\n=== VARIABLE BYTE ENCODING ===\")\n",
    "print(\"Encoded Bytes (Decimal):\", vb_encoded)\n",
    "print(\"Encoded Bytes (Hex):\", [hex(b) for b in vb_encoded])\n",
    "print(\"Encoded Bytes (Binary):\", [format(b, '08b') for b in vb_encoded])\n",
    "\n",
    "vb_bits = len(vb_encoded) * 8\n",
    "print(f\"Total Bits Used: {vb_bits}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc67a5bd",
   "metadata": {},
   "source": [
    "### 3. Golomb Encoding\n",
    "\n",
    "Golomb encoding is a lossless compression technique suitable for sequences of\n",
    "geometrically distributed integers, such as gap values in posting lists.\n",
    "\n",
    "For a number x and parameter M:\n",
    "- Quotient q = ⌊x / M⌋ is encoded using Unary coding\n",
    "- Remainder r = x mod M is encoded using Binary coding\n",
    "\n",
    "Golomb encoding is particularly effective when the average gap is small.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b1a07a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== GOLOMB ENCODING ===\n",
      "Chosen M: 1\n",
      "Encoded Bit Stream: 1001100\n",
      "Total Bits Used: 7\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def golomb_encode(gaps, M):\n",
    "    b = math.ceil(math.log2(M))\n",
    "    cutoff = 2**b - M\n",
    "    bit_stream = \"\"\n",
    "\n",
    "    for x in gaps:\n",
    "        q = x // M\n",
    "        r = x % M\n",
    "\n",
    "        # Unary encoding of quotient\n",
    "        bit_stream += \"1\" * q + \"0\"\n",
    "\n",
    "        # Binary encoding of remainder\n",
    "        if r < cutoff:\n",
    "            bit_stream += format(r, f\"0{b-1}b\")\n",
    "        else:\n",
    "            bit_stream += format(r + cutoff, f\"0{b}b\")\n",
    "\n",
    "    return bit_stream\n",
    "\n",
    "\n",
    "mean_gap = sum(gaps) / len(gaps)\n",
    "M = max(1, int(0.69 * mean_gap))\n",
    "\n",
    "golomb_bits = golomb_encode(gaps, M)\n",
    "\n",
    "print(\"\\n=== GOLOMB ENCODING ===\")\n",
    "print(f\"Chosen M: {M}\")\n",
    "print(\"Encoded Bit Stream:\", golomb_bits)\n",
    "print(f\"Total Bits Used: {len(golomb_bits)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "70e218ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== COMPRESSION COMPARISON SUMMARY ===\n",
      "----------------------------------------\n",
      "Original (32-bit integers): 64 bits\n",
      "Gap Encoding Only:          3 bits\n",
      "Variable Byte Encoding:     16 bits\n",
      "Golomb Encoding:            7 bits\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== COMPRESSION COMPARISON SUMMARY ===\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Original (32-bit integers): {original_bits} bits\")\n",
    "print(f\"Gap Encoding Only:          {gap_bits} bits\")\n",
    "print(f\"Variable Byte Encoding:     {vb_bits} bits\")\n",
    "print(f\"Golomb Encoding:            {len(golomb_bits)} bits\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
